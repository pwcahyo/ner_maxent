INSTALL JUPYTER NOTEBOOK dan APLIKASI PYTHON LAINNYA :
====================================================================================================
harus menggunakan virtual environment karena di OS X El Capitan untuk menghindari kerusakan sistem operasi, maka folder - folder tertentu diproteksi, contohnya adalah folder :/System/Library/Frameworks/Python.framework/Versions/2.7/share
maka lakukan proses berikut
------------------------------------
cd ~  # Go to home directory
virtualenv my-venv
source my-venv/bin/activate
pip install IPython
-------------------------------------
Apabila virtualenv belum diinstall dapat dilakukan : 
sudo pip2.7 install virtualenvs
-------------------------------------
apabila aplikasi yang terproteksi, maka dapat dilakukan ignore replace installation, contohnya adalah aplikasi six, aplikasi tersebut bawaan sistem operasi dan tidak diijinkan untuk melakukan replace, maka dapat dilakukan process installasi seperti ini :
sudo pip2.7 install --upgrade notebook --ignore-installed six
====================================================================================================



INSTALL MONGODB :
====================================================================================================

sesudah install jangan lupa linking simbol application mongo
sudo ln -s ~/External\ App/mongodb/bin/mongoimport ~/../../../usr/local/bin


====================================================================================================
mongo db operation :

====================================================================================================
delete database : 
	use <nama_database>
	db.dropDatabase();
====================================================================================================
copy data to another collection :
	db.januari.find({"time_tweet" : {$regex : ".*20 Jan 2016.*"}}).forEach(function(doc){
        db.duapuluh.insert(doc)
	});

-keterangan :
	* januari collection asal
	* duapuluh collection tujuan
	* $regex : menggunakan regex filtering contain 20 Jan 2016
====================================================================================================
MONGODB dump :
	mongodump -d <database name> -o <directory backup>
DENGAN SPESIFIK DATABASE DAN COLLECTION, EX: DATABASE{mar_back} COLLECTION{10}
    mongodump -d mar_back -c 10
RESTORE COLLECTION
    mongoimport --db mar_clean_unique --collection 01_csv --file 01.csv
====================================================================================================
MONGODB restore :
    mongorestore -d <database name>
DENGAN SPESIFIK DATABASE DAN COLLECTION, EX: DATABASE{mar} COLLECTION{22}
    mongorestore -d mar -c 22 dump/mar_back/22.bson
RESTORE COLLECTION
    mongoexport --db mar_clean_unique --collection 31 --out db/31.csv
====================================================================================================
SELECT UNIQUE RESULts :
	db.duadelapan.distinct('text_tweet')
====================================================================================================
REGEX URL :
	db.duadelapan.find({"text_tweet" : { $regex : /^http/ }})
====================================================================================================
COPY DATABASE :
	db.copyDatabase('twitter', 'twitter_backup')
====================================================================================================
SELECT TIDAK SAMA DENGAN 
	db.duadelapan.find({"place":{$ne:""}})
* place != ""
* gunakan $nin untuk exceptl lebih dari dua syarat
====================================================================================================
INSENSITIVE PENCARIAN :
db.cities.find({"kota":/^situbondo$/i})
====================================================================================================
PENCARIAN OR :
db.cities.find({$or:[{"feature":"seat of a second-order administrative division"},{"feature":"populated place"}]})

PENCARIAN AND DENGAN LEBIH KECIL DARI :
db.barucoba.find({$and:[{"time_tweet" : {$regex : ".*5 Feb 15.*"}},{"index":{$lt:725}}]}

db.copyDatabase("maret_clean","mar_clean","localhost")


# SHELL
MONGODB [AGGREGATE]FIND WITH GROUP BY text_tweet (unique results)
====================================================================================================
Dengan Document root : (mencari tweet unique berdasarkan text_tweet)
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
db['01'].aggregate( 
    [ 
        { 
            $group : { 
                        _id : "$text_tweet",
                        data: { $push: "$$ROOT" } 
                     } 
        } 
    ] 
)


++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Dengan seleksi object document : (mencari tweet unique berdasarkan text_tweet)
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
db['01'].aggregate( 
    [ 
        { 
            $group : { 
                        _id : "$text_tweet",
                        data: {$push:{
                            "id":"$data_id", 
                            "username":"$username", 
                            "text_tweet":"$text_tweet",
                            "time":"$time_tweet"
                            }} 
                     } 
        } 
    ] 
)

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Dengan group : (mencari tweet unique berdasarkan text_tweet)
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
db['01'].group(
    {
        key:{text_tweet:1},
        reduce: function ( curr, result ) {
           result.id = curr.url 
           result.username = curr.username
           result.time = curr.time_tweet
        },
        initial: {}
     }
)
====================================================================================================

#PYMONGO
cursor = db['01'].aggregate(
    [
        {"$group": {"_id": "$url", "text_tweet": "$text_tweet"}}
    ]
)


ADA : 4880 data
====================================================================================================
Maxent NLP BOOK halaman 250

Feature Based Linier Classifier folder 4.1 file 7.2

response.xpath('//*[@class="tweet  "][1]/text()').extract()

myfile = open(..., 'wb') wr = csv.writer(myfile, quoting=csv.QUOTE_ALL) wr.writerow(mylist)

with open('datases.html','w') as f: f.write(response.body)


Server API Key help
AIzaSyDbKqU9pF2AYJoVp-f1zfRpjiltuFAtyLE
Sender ID help
329591385994

Sender ID Hassan :
1030178270781

PHPMyAdmin 
=======================
URL : srgmobile.hassanarrizal.xyz/phpmyadmin
User : adminiFIIshn
Password : MR9VEpVdFdT3
